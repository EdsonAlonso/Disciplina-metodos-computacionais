{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a set of measurements $d_{i}$, $i = 1, \\dots, N$, of a given physical quantity. The measurements $d_{i}$ are usually called observed data. Let's also consider that each observed data $d_{i}$ can be properly approximated by a function $y_{i}$, $i = 1, \\dots, N$, given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_{i} = a_{i1} \\, x_{1} + a_{i2} \\, x_{2} + \\cdots + a_{iM} \\, x_{M} \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $a_{ij}$ are known variables and $x_{j}$ are unknown variables, $i = 1, \\dots, N$, $j = 1, \\dots, M$, $N \\gt M$. The $y_{i}$ are usually called predicted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by considering the $N$ measurements, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "d_{1} &\\approx \\; &y_{1} &= \\: &a_{11} \\, x_{1} + &a_{12} \\, x_{2} + \\cdots + &a_{1M} \\, x_{M} \\\\\n",
    "d_{2} &\\approx &y_{2} &= &a_{21} \\, x_{1} + &a_{22} \\, x_{2} + \\cdots + &a_{2M} \\, x_{M} \\\\\n",
    "\\vdots & &\\vdots & & \\: \\vdots & \\: \\vdots & \\: \\vdots \\\\\n",
    "d_{N} &\\approx &y_{N} &= &a_{N1} \\, x_{1} + &a_{N2} \\, x_{2} + \\cdots + &a_{NM} \\, x_{M}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, equivalently,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{d} \\approx \\mathbf{y} = \\mathbf{A} \\, \\mathbf{x} \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x} = \n",
    "\\left[ \\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{M}\n",
    "\\end{array} \\right] \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \n",
    "\\left[ \\begin{array}{c}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{N}\n",
    "\\end{array} \\right] \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{d} = \n",
    "\\left[ \\begin{array}{c}\n",
    "d_{1} \\\\\n",
    "d_{2} \\\\\n",
    "\\vdots \\\\\n",
    "d_{N}\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} = \n",
    "\\left[ \\begin{array}{cccc}\n",
    "a_{11} & a_{12} & \\cdots & a_{1M} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2M} \\\\\n",
    "\\vdots & \\vdots &        & \\vdots \\\\\n",
    "a_{N1} & a_{N2} & \\cdots & a_{NM}\n",
    "\\end{array} \\right] \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, to determine the vector $\\mathbf{x}$ from $\\mathbf{d}$ and $\\mathbf{A}$, it is needed to solve a non-square linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider the problem of determining the vector $\\mathbf{x}$ from the measurements $\\mathbf{d}$ and the $N \\times M$ matrix $\\mathbf{A}$. Mathematically, this problem consists in determining a vector $\\mathbf{x} = \\mathbf{x}^{\\ast}$ producing a $\\mathbf{y}$ *\"as close as possible\"* to $\\mathbf{d}$. To solve this problem, we need to define what *\"as close as possible\"* means. The notion of \"*closeness*\" is intrinsically related to the notion of \"*distance*\" and, consequently, to the notion of <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\">norm</a>.\n",
    "\n",
    "For example, let's consider a vector $\\mathbf{r} = \\mathbf{d} - \\mathbf{y}$, which is defined as the difference between the observed data $\\mathbf{d}$ and the predicted data $\\mathbf{y}$. The length of this vector can be determined by the following scalar function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\| \\mathbf{r} \\|_{2} &= \\sqrt{\\mathbf{r}^{\\top}\\mathbf{r}} \\\\\n",
    "&= \\sqrt{\\sum \\limits_{i = 1}^{N} r_{i}^{2}}\n",
    "\\end{split} \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $r_{i} = d_{i} - y_{i}$. This function is a norm that quantifies the \"*distance*\" between the vectors $\\mathbf{d}$ and $\\mathbf{y}$. It is called <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm\">*\"Euclidean norm\"*</a> or *\"Euclidean distance\"*. Notice that this function equals to zero if $\\mathbf{d} = \\mathbf{y}$ and it is greater than zero if $\\mathbf{d} \\ne \\mathbf{y}$.\n",
    "\n",
    "So, the problem of determining a vector $\\mathbf{x} = \\mathbf{x}^{\\ast}$ producing a $\\mathbf{y}$ *\"as close as possible\"* to $\\mathbf{d}$ can be thought of the problem of determining a vector $\\mathbf{x} = \\mathbf{x}^{\\ast}$ producing the minimum Euclidean norm of the difference between the observed data $\\mathbf{d}$ and the predicted data $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical situations, instead of determining the vector $\\mathbf{x}^{\\ast}$ minimizing the Euclidean norm of $\\mathbf{r}$, we determine the vector $\\mathbf{x}^{\\ast}$ minimizing the *\"Squared Euclidean norm\"* (<a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">*\"Squared Euclidean distance\"*</a>) of $\\mathbf{r}$, which is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\| \\mathbf{r} \\|_{2}^{2} &= \\mathbf{r}^{\\top}\\mathbf{r} \\\\\n",
    "&= \\sum \\limits_{i = 1}^{N} r_{i}^{2}\n",
    "\\end{split} \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Squared Euclidean norm of $\\mathbf{r}$ is a scalar function depending on the unknows $\\mathbf{x}$ and can be written as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Phi(\\mathbf{x}) = \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right]^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering that there is only one vector $\\mathbf{x}^{\\ast}$ producing the minimum $\\Phi(\\mathbf{x})$, we can state that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Phi(\\mathbf{x}^{\\ast} + \\Delta \\mathbf{x})\n",
    "\\begin{cases}\n",
    "\\gt \\Phi(\\mathbf{x}^{\\ast}) \\, , \\: \\text{if} \\:\\: \\| \\Delta \\mathbf{x}\\|_{2} \\ne 0 \\\\\n",
    "= \\Phi(\\mathbf{x}^{\\ast}) \\, , \\: \\text{if} \\:\\: \\| \\Delta \\mathbf{x}\\|_{2} = 0 \\\\\n",
    "\\end{cases} \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides that, it can be proven that the vector $\\mathbf{x}^{\\ast}$ satisfies the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla \\Phi(\\mathbf{x}^{\\ast}) = \n",
    "\\left[ \\begin{array}{c}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array} \\right] \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla \\Phi(\\mathbf{x}) = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{x})}{\\partial \\, x_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{x})}{\\partial \\, x_{M}} \n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the gradient of $\\Phi(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $j$ th element of the gradient $\\nabla \\Phi(\\mathbf{x})$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial \\, \\Phi(\\mathbf{x})}{\\partial \\, x_{j}} &= \n",
    "\\Big \\{ \\dfrac{\\partial}{\\partial \\, x_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\Big \\} \n",
    "^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] +\n",
    "\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right]^{\\top}\n",
    "\\Big \\{ \\dfrac{\\partial}{\\partial \\, x_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\Big \\} \\\\\n",
    "&= 2\\Big \\{ \\dfrac{\\partial}{\\partial \\, x_{j}} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\Big \\} \n",
    "^{\\top}\\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\\\\n",
    "&= 2 \\Big \\{ -\\mathbf{u}_{j}^{\\top}\\mathbf{A}^{\\top} \\Big \\} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{u}_{j}$ is a $M \\times 1$ vector whose $j$ th element is equal to $1$ and all the remaining elements are equal to $0$. By substituting this $\\dfrac{\\partial \\, \\Phi(\\mathbf{x})}{\\partial \\, x_{j}}$ into the gradient equation, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla \\Phi(\\mathbf{x}) = -2 \\mathbf{A}^{\\top} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x} \\right] \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by evaluating the gradient $\\nabla \\Phi(\\mathbf{x})$ at $\\mathbf{x} = \\mathbf{x}^{\\ast}$, we obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\nabla \\Phi(\\mathbf{x}^{\\ast}) &= -2 \\mathbf{A}^{\\top} \\left[ \\mathbf{d} - \\mathbf{A}\\mathbf{x}^{\\ast} \\right] \\\\\n",
    "\\mathbf{0} &= -\\mathbf{A}^{\\top}\\mathbf{d} + \\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{x}^{\\ast} \n",
    "\\end{split} \\: ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{x}^{\\ast} = \\mathbf{A}^{\\top}\\mathbf{d} \\: .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation is commonly called Least Squares Estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, to determine the vector $\\mathbf{x}^{\\ast}$, it is needed to solve a square linear system with a symmetric matrix $\\mathbf{A}^{\\top}\\mathbf{A}$ and an independent vector $\\mathbf{A}^{\\top}\\mathbf{d}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
